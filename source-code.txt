
Source code:
# %% [markdown]
# 
# # TIỀN XỬ LÝ DỮ LIỆU (PHẦN 1)

# %% [markdown]
# Trong lĩnh vực machine learning, việc tiền xử lý dữ liệu đóng vai trò quan trọng để xây dựng những mô hình **chính xác và mạnh mẽ**. Dữ liệu thô thường gặp phải tình trạng lộn xộn, thiếu sót và không nhất quán. Tiền xử lý dữ liệu bao gồm một loạt các kỹ thuật và nhiệm vụ nhằm **biến đổi và tinh chỉnh dữ liệu thô** thành một định dạng thích hợp, chuẩn bị cho việc phân tích và huấn luyện mô hình.
# 
# Bằng cách giải quyết các vấn đề như giá trị thiếu, các ngoại lệ và tỷ lệ biến đổi khác nhau, việc tiền xử lý đảm bảo **chất lượng dữ liệu được cải thiện, dẫn đến hiệu suất mô hình cải thiện**.
# 
# Trong cuốn sổ tay Jupyter này, chúng ta sẽ đào sâu vào các bước cơ bản của tiền xử lý dữ liệu. Chúng ta sẽ làm việc thông qua một bài tập thực tế bằng Python, nơi chúng ta sẽ tập trung vào 2 nhiệm vụ cơ bản:
# 
# * Xác định và xử lý giá trị bị **thiếu**
# * **Chuẩn hóa dữ liệu**
# 
# Khi kết thúc bài tập này, bạn sẽ đã có được kinh nghiệm thực tế trong việc đánh giá, làm sạch và biến đổi dữ liệu thô thành một định dạng thích hợp cho việc học máy. Những kỹ năng này là nền tảng trong hành trình khoa học dữ liệu, vì chúng đặt ra sân khấu cho các kỹ thuật nâng cao hơn và việc xây dựng mô hình. Vậy thì, hãy bắt đầu và học nghệ thuật tiền xử lý dữ liệu để mở khóa tiềm năng thực sự của những nỗ lực trong lĩnh vực học máy của bạn.

# %% [markdown]
# ### 1. Tải dữ liệu

# %% [markdown]
# Dữ liệu được sử dụng sẽ là dữ liệu về giá nhà, cụ thể như sau:

# %%
def get_driveurl(file_id):
  return f"https://drive.google.com/uc?id={file_id}"

# %%
import pandas as pd

data_path = get_driveurl('11rBtXktvl0BJ79tLBNhaYlWnS46caZeO')
df = pd.read_csv(data_path, index_col=0)
df_test = df.copy()
df.head()

# %% [markdown]
# ### 2. Xác định số phần tử bị thiếu ở mỗi cột

# %%
# Liệt kê số phần tử bị thiếu ở mỗi cột
df.isna().sum()

# %% [markdown]
# ### 3. Loại bỏ những cột có nhiều giá trị bị thiếu

# %% [markdown]
# **Bài tập**: Hãy viết hàm nhận vào dataframe và threshold (ngưỡng phần trăm).
# Trả về dataframe mới sau khi đã loại bỏ hết tất cả các cột mà tỉ lệ phần
# trăm giá trị bị thiếu vượt qua threshold.

# %%
def drop_sparse_columns(df: pd.DataFrame, threshold: float) -> pd.DataFrame:


    # BEGIN SOLUTION
    missing_percentage = df.isnull().sum() / len(df)
    columns_to_drop = missing_percentage[missing_percentage > threshold].index
    df_dropped = df.drop(columns=columns_to_drop)
    return df_dropped
    # END SOLUTION
    pass

# %% [markdown]
# Ta tiến hành loại bỏ những cột có nhiều giá trị bị thiếu

# %%
# Nếu cột có phần trăm giá trị bị thiếu > 60% thì sẽ bị loại bỏ
threshold = 0.6
df = drop_sparse_columns(df, threshold)

# %%
# Kiểm tra với public tests
assert df.shape[1] == 6

# %%
# Sau khi đã loại bỏ những cột không cần thiết do chứa quá nhiều giá trị bị thiếu
df.isna().sum()

# %% [markdown]
# ### 4. Lắp đầy những giá trị thiếu ở những cột còn lại

# %% [markdown]
# **Bài tập**: Hãy viết các hàm thực hiện điền giá trị bị thiếu vào dataframe ứng với
# với các chiến lược sau: ***min imputation***, ***max imputation***, ***mean imputation***, ***zero imputation***.

# %%
df.isna()

# %%
df.min()

# %%
def fill_with_min(df: pd.DataFrame) -> pd.DataFrame:

    ### BEGIN SOLUTION
    return df.fillna(df.min())
    ### END SOLUTION
    pass

def fill_with_max(df: pd.DataFrame) -> pd.DataFrame:

    ### BEGIN SOLUTION
    return df.fillna(df.max())
    ### END SOLUTION
    pass

def fill_with_mean(df: pd.DataFrame) -> pd.DataFrame:

    ### BEGIN SOLUTION
    return df.fillna(df.mean())
    ### END SOLUTION
    pass

def fill_with_zero(df: pd.DataFrame) -> pd.DataFrame:

    ### BEGIN SOLUTION
    return df.fillna(0)
    ### END SOLUTION
    pass

# %% [markdown]
# Ta gọi hàm và tạo những dataframe mới ứng với từng kiểu điền rỗng

# %%
min_filled_df = fill_with_min(df)
max_filled_df = fill_with_max(df)
mean_filled_df = fill_with_mean(df)
zero_filled_df = fill_with_zero(df)

# %%
# Kiểm tra với public tests
assert not min_filled_df.isna().any().any()
assert not max_filled_df.isna().any().any()
assert not mean_filled_df.isna().any().any()
assert not zero_filled_df.isna().any().any()

# %% [markdown]
# ## 4. Chuẩn hóa dữ liệu
# Các đặc trưng thường đi kèm với các **tỷ lệ biến đổi (range)** khác nhau, điều này có thể dẫn đến mô hình **thiên vị**. Chúng ta sẽ khám phá các kỹ thuật chuẩn hóa phổ biến
# 
# - Min-Max Scaling: Nó biến đổi các giá trị trong tập dữ liệu về các giá trị trong khoảng từ 0 đến 1.
# $$ x_{scaled} = {x-x_{min} \over x_{max} - x_{min}} $$
# 
# 
# >>>| x | $x_{scaled}$ |
# |:--------:|:--------:|
# | 10       | 0.0      |
# | -20       | 0.5      |
# | 35       | 0.25      |
# | 48       | 1.0      |
# | 53       | 0.75      |
# 
# - Standard Scaling (Z-score normalization): Nó tính toán giá trị trung bình và độ lệch chuẩn của tập dữ liệu và chuẩn hóa nó bằng cách trừ giá trị trung bình và chia cho độ lệch chuẩn.
# 
# $$ x_{scaled} = {x- mean_x \over std_x} $$
# 
# >>>| x | $x_{scaled}$ |
# |:--------:|:--------:|
# | 10       | -0.56     |
# | -20       | -1.67      |
# | 35       | 0.36      |
# | 48       | 0.84     |
# | 53       | 1.03      |
# 
#  >>>$mean_x=$25.2, $std_x \approx$27.0658
# 
#  >>>$mean_{x_{scaled}} \approx$0, $std_{x_{scaled}} \approx$1
# 
# - Robust Scaling: RobustScaler là một kỹ thuật sử dụng trung vị và quartiles để giải quyết các bias từ các giá trị ngoại lệ.
# 
# $$ x_{scaled} = {x-x_{median} \over x_{75} - x_{25}} $$
# 
# >>>| x | $x_{scaled}$ |
# |:--------:|:--------:|
# | 10       | -0.66     |
# | -20       | -1.45      |
# | 35       | 0.0      |
# | 48       | 0.34     |
# | 53       | 0.47      |
# 
# ![anh](https://i.imgur.com/MARX2bg.png)
# 
#  Những kỹ thuật này sẽ giúp đưa các đặc trưng về một tỷ lệ chung, ngăn chặn bất kỳ đặc trưng nào chiếm ưu thế trong quá trình học.

# %% [markdown]
# **Bài tập**: Hãy viết hàm nhận vào dataframe và một object thuộc một trong ba
# scaler đã được import bên dưới và trả vể dataframe đã được chuẩn hóa sử dụng scaler đó.

# %%
# Sử dụng các class scaler có từ thư viện sklearn
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
import numpy as np

# %%
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Tạo DataFrame với một cột dữ liệu
data = {'Column1': [10, -20, 35, 48, 53]}
df = pd.DataFrame(data)

# Khởi tạo StandardScaler
scaler = RobustScaler()

# Áp dụng StandardScaler vào cột dữ liệu
scaled_data = df.apply(lambda col: scaler.fit_transform(col.values.reshape(-1, 1)).flatten())

# In kết quả
print(scaled_data)

# %%
mean_filled_df.info()

# %%
def scale(df: pd.DataFrame, scaler) -> pd.DataFrame:

    ### BEGIN SOLUTION
    # Tạo một bản sao của DataFrame để tránh sửa đổi DataFrame ban đầu
    df_scaled = df.copy()
    # Lặp qua từng cột trong DataFrame
    for column in df_scaled.columns:
        # Reshape cột thành một mảng 2D vì scaler yêu cầu đầu vào 2D
        values = df_scaled[column].values.reshape(-1, 1)

        # Khớp scaler với dữ liệu và biến đổi nó
        scaled_values = scaler.fit_transform(values)

        # Gán các giá trị đã được chia tỷ lệ trở lại cột trong DataFrame
        df_scaled[column] = scaled_values

    # Trả về DataFrame đã được chia tỷ lệ
    return df_scaled
    ### END SOLUTION
    pass

# %% [markdown]
# Tiến hành tạo các dataframe ứng với từng kiểu chuẩn hóa

# %%
minmax_scaled_df = scale(mean_filled_df, MinMaxScaler())
standard_scaled_df = scale(mean_filled_df, StandardScaler())
robust_scaled_df = scale(mean_filled_df, RobustScaler())

# %% [markdown]
# <h1><b>Tiền xử lí dữ liệu (phần 2)

# %% [markdown]
# Trong bài này, ta sẽ tiếp tục thực hành những vấn đề:
# 
#     - Data transformation:
#         + Kỹ thuật Ordinal Encoding và One-hot encoding
#         + Kỹ thuật binning: Uniform, Quantile, Clustered
#     - Data augmentation: tăng cường dữ liệu để đề phòng trường hợp dữ liệu mất cân đối hoặc các mẫu / phân lớp quá hiếm gặp

# %% [markdown]
# <h1>Data Transformation

# %% [markdown]
# <h3>Kĩ thuật Ordinal Encoding

# %% [markdown]
# Đầu tiên ta cần import thư viện cần thiết pandas với tên tắt là pd

# %%
### BEGIN SOLUTION
import pandas as pd 
### END SOLUTION

# %% [markdown]
# Tiếp đến là đọc file csv bằng pandas như những bài trước và lưu dataframe vào biến df. Khi nộp trên hệ thống, các bạn vui lòng đọc tại đường dẫn data/heart.csv.

# %%
### BEGIN SOLUTION
df = pd.read_csv("data/heart.csv")
### END SOLUTION
df.head()

# %% [markdown]
# Ta thấy ở cột ChestPain, cột Thal, cột AHD có dữ liệu dạng **category**.
# Giả sử mô hình của ta chỉ nhận dữ liệu đầu vào dạng số, ta phải biến các cột này thành dạng số sao cho những đặc trưng này không bị thay đổi
# 
# Ở cột ChestPain:
# *   typical: 1
# *   asymptomatic: 2
# *   nonanginal: 3
# *   nontypical: 4
# 
# Ở cột Thal:
# *   normal: 1
# *   fixed: 2
# *   reversable: 3
# 
# Ở cột AHD:
# *   No: 0
# *   Yes: 1

# %%
### BEGIN SOLUTION
import pandas as pd


chest_pain_map = {
    'typical': 1,
    'asymptomatic': 2,
    'nonanginal': 3,
    'nontypical': 4
}
df['ChestPain'] = df['ChestPain'].map(chest_pain_map)

thal_map = {
    'normal': 1,
    'fixed': 2,
    'reversable': 3
}
df['Thal'] = df['Thal'].map(thal_map)

ahd_map = {
    'No': 0,
    'Yes': 1
}
df['AHD'] = df['AHD'].map(ahd_map)
### END SOLUTION

df.head()

# %%
assert df['ChestPain'].dtypes == 'int64'
assert df.shape == (50, 15)

# %% [markdown]
# <h3>Kĩ thuật One-hot encoding

# %% [markdown]
# Ở đây ta sẽ tiếp tục đi đến một kĩ thuật khác đó là One-hot

# %% [markdown]
# Ta tiếp tục đọc file csv bằng pandas và lưu dataframe vào biến df. Khi nộp trên hệ thống, các bạn vui lòng đọc tại đường dẫn data/heart.csv.

# %%
### BEGIN SOLUTION
df = pd.read_csv("data/heart.csv")
### END SOLUTION
df.head()

# %% [markdown]
# Ở cột Thal, ta sẽ biến cột này thành One-hot bằng cách thêm 3 cột fixed, normal, reversable vào df
# 
# **Ba cột mới này sẽ biểu diễn One-hot cho cột Thal cũ**

# %%
### BEGIN SOLUTION
df = pd.get_dummies(df, columns=["Thal"], prefix="", prefix_sep="")
#!!! Thal be drop 

### END SOLUTION

df

# %%
assert df.shape == (50, 17)
assert df['fixed'][0] == 1
assert df['normal'][0] == 0
assert df['reversable'][0] == 0

# %% [markdown]
# <i>Trong một số trường hợp, ta không thể giữ nguyên dữ liệu dạng số để tính toán, mà ta cần chuyển nó qua dạng category
# 
# Thường thì các mô hình Tree sẽ cần dữ liệu dạng Category </i>
# 
# Để làm được việc này, ta sẽ đi đến kĩ thuật đầu tiên Uniform

# %% [markdown]
# <h3>Kĩ thuật Uniform

# %%
import matplotlib.pyplot as plt 

plt.plot(df['Age'])
plt.xlabel('Index')
plt.ylabel('Age');

# %% [markdown]
# **Kĩ thuật Uniform dùng để biến dữ liệu thành các Bin có khoảng dữ liệu bằng nhau**
# 
# Mục tiêu: Tạo thêm **1 cột**, giá trị của cột là middle-age hoặc old hoặc elderly, cột có label là Uniform, cột này ở kế sau cột Age
# 
# Ta có thể làm thủ công hoặc sử dụng phương thức qcut của pandas

# %%
### BEGIN SOLUTION
col = pd.qcut(df['Age'], q=3, labels=['middle-age', 'old', 'elderly'])
# type(col)
index = df.columns.to_list().index("Age")
df.insert(2, "Uniform", col)

### END SOLUTION

# %%
import seaborn as sns
sns.countplot(x=df['Uniform']);

# %% [markdown]
# Ta có thể thấy số lượng dữ liệu chênh nhau không quá nhiều

# %%
assert df.columns[2] == 'Uniform'
assert df['Uniform'].dtype == 'category'

# %% [markdown]
# <h3>Kĩ thuật Quantile

# %% [markdown]
# Kĩ thuật Quantile dùng để chia dữ liệu thành những khoảng với phần trăm cho trước
# 
# Giả sử ta sẽ chia cột Age thành 4 phần:
# * Q1 <= 25%
# * 25% < Q2 <= 50%
# * 50% < Q3 <= 95%
# * 95% < Q4
# 
# Sau đó ta thêm cột Quantile với các giá trị Q1, Q2, Q3, Q4 ở vị trí thứ 3 của dataframe df

# %%
labels = ['Q1', 'Q2', 'Q3', 'Q4']
quantiles = df['Age'].quantile([0.25, 0.5, 0.95])
### BEGIN SOLUTION
df.insert(3, 'Quantile', pd.qcut(df['Age'], q=[0, 0.25, 0.5, 0.95, 1], labels=labels))
### END SOLUTION

# %%
sns.countplot(x=df['Quantile']);

# %%
assert df.shape == (50, 19)

# %% [markdown]
# <h3>Kĩ thuật Clustered

# %% [markdown]
# Kĩ thuật Clustered dùng để biến dữ liệu theo ý của ta
# 
# Ở đây là các tuổi lớn hơn **60** là C1 và còn lại là C2
# 
# Ta tiếp tục thêm cột Clustered ở vị trí thứ 4, giá trị của cột này là C1, C2

# %%
labels = ['C1', 'C2']
### BEGIN SOLUTION
df.insert(4, 'Clustered', df['Age'].apply(lambda x: 'C1' if x > 60 else 'C2'))
### END SOLUTION

# %%
sns.countplot(x=df['Clustered']);

# %% [markdown]
# <h1>Data augmentation

# %% [markdown]
# Đối với những bài toán mà quá ít dữ liệu, ta có thể tìm thêm hoặc tạo ra dữ liệu mới từ những dữ liệu hiện có
# 
# Nhưng điều này ít nhiều cũng sẽ ảnh hưởng đến chất lượng của mô hình, ta nên cân nhắc kĩ khi sử dụng cách này

# %% [markdown]
# # EDA: PHÂN TÍCH ĐƠN BIẾN
# 
# **Phân tích đơn biến** là một phương pháp cơ bản trong phân tích dữ liệu, liên quan đến việc nghiên cứu và tổng hợp một biến duy nhất tại một thời điểm. Nó giúp chúng ta hiểu về phân phối, trung bình và biến đổi của từng biến trong tập dữ liệu.
# 
# Trong Jupyter Notebook này, chúng ta sẽ tiến hành **phân tích đơn biến** trên **bộ dữ liệu Iris**. Bộ dữ liệu Iris là một bộ dữ liệu nổi tiếng, chứa thông tin về bốn thuộc tính: chiều dài đài hoa, chiều rộng đài hoa, chiều dài cánh hoa và chiều rộng cánh hoa, cho ba loại hoa iris khác nhau: setosa, versicolor và virginica. Mục tiêu của phân tích này là khám phá từng thuộc tính một cách độc lập và biểu đạt đặc điểm của chúng bằng cách sử dụng nhiều loại biểu đồ khác nhau.
# 
# Trong suốt notebook này, sẽ hướng dẫn bạn qua một số bài tập, mỗi bài tập tập trung vào một khía cạnh cụ thể của phân tích đơn biến:
# 1. Vẽ biểu đồ histogram về chiều dài đài hoa.
# 2. Tạo biểu đồ hộp (box plot) cho chiều rộng cánh hoa.
# 3. Tạo biểu đồ mật độ ước tính hạt nhân (KDE) cho chiều rộng đài hoa.
# 4. Tạo biểu đồ đếm cho biến "target".
# 5. Xây dựng biểu đồ violon cho chiều dài cánh hoa.
# 
# Hãy cùng bắt đầu với các bài tập và khám phá những thông tin ẩn trong bộ dữ liệu Iris!
# 

# %% [markdown]
# ### 1. Tải dữ liệu

# %%
# Import những thư viện cần thiết
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from PIL import Image
from io import BytesIO

# %%
# Tải dữ liệu hoa Iris
iris = load_iris()

df = pd.DataFrame(
    data=np.c_[iris["data"], iris["target"]],
    columns=iris["feature_names"] + ["target"]
)
# Xem qua dữ liệu
df.head()

# %% [markdown]
# ### 2. Bài tập vẽ biểu đồ

# %% [markdown]
# **Bài tập**: Hãy sử dụng thư viện sns để tạo một histogram đối với feature ***sepal length (cm)***
# với các tham số như sau:
# 
# - ***figsize***: (8, 6)
# - ***title***: "Histogram of Sepal Length"
# - ***xlabel***: "Sepal Length (cm)"
# - ***ylable***: "Frequency"
# 
# Lưu ý: Không thêm bất kỳ tùy chỉnh nào khác cho biểu đồ để thuận tiện cho việc chấm tự động.

# %%
plt.figure(figsize=(8, 6))

### BEGIN SOLUTION
sns.histplot(df['sepal length (cm)'])  # Tạo histogram
plt.title("Histogram of Sepal Length")
plt.xlabel("Sepal Length (cm)")
plt.ylabel("Frequency")

### END SOLUTION

plt.savefig('part_1.png')

# %% [markdown]
# **Bài tập**: Hãy sử dụng thư viện sns để tạo một box plot đối với feature ***petal width (cm)***
# với các tham số như sau:
# 
# - ***figsize***: (8, 6)
# - ***title***: "Box Plot of Petal Width"
# - ***ylable***: "Petal Width (cm)"
# 
# Lưu ý: Không thêm bất kỳ tùy chỉnh nào khác cho biểu đồ để thuận tiện cho việc chấm tự động.

# %%
plt.figure(figsize=(8, 6))

### BEGIN SOLUTION
sns.boxplot(y='petal width (cm)', data=df)
plt.title("Box Plot of Petal Width")
plt.ylabel("Petal Width (cm)")
### END SOLUTION

plt.savefig('part_2.png')

# %% [markdown]
# **Bài tập**: Hãy sử dụng thư viện seaborn để tạo một kernel density plot (kde) đối với feature ***sepal width (cm)*** với các tham số như sau:
# 
# - ***figsize***: (8, 6)
# - ***title***: "Kernel Density Estimate of Sepal Width"
# - ***xlabel***: "Sepal Width (cm)"
# - ***ylabel***: "Density"
# 
# Lưu ý: Không thêm bất kỳ tùy chỉnh nào khác cho biểu đồ để thuận tiện cho việc chấm tự động.

# %%
plt.figure(figsize=(8, 6))

### BEGIN SOLUTION
sns.kdeplot(df["sepal width (cm)"])
plt.title("Kernel Density Estimate of Sepal Width")
plt.xlabel("Sepal Width (cm)")
plt.ylabel("Density")


### END SOLUTION

plt.savefig('part_3.png')

# %% [markdown]
# **Bài tập**: Hãy sử dụng thư viện seaborn để tạo count plot đối với feature ***target***
# với các tham số như sau:
# - ***figsize***: (8, 6)
# - ***title***: "Count Plot of Target Classes"
# - ***xlabel***: "Target Class"
# - ***ylabel***: "Count"
# Lưu ý: Không thêm bất kỳ tùy chỉnh nào khác cho biểu đồ để thuận tiện cho việc chấm tự động.

# %%
plt.figure(figsize=(8, 6))

### BEGIN SOLUTION
sns.countplot(x='target', data=df)
plt.title("Count Plot of Target Classes")
plt.xlabel("Target Class")
plt.ylabel("Count")


### END SOLUTION

plt.savefig('part_4.png')

# %% [markdown]
# **Bài tập**: Hãy sử dụng thư viện seaborn để tạo violin plot cho feature ***petal length*** cho mỗi feature ***target*** với các tham số như sau:
# 
# - ***figsize***: (8, 6)
# - ***title***: "Violin Plot of Petal Length"
# - ***ylabel***: "Petal Length (cm)"
# 
# Lưu ý: Không thêm bất kỳ tùy chỉnh nào khác cho biểu đồ để thuận tiện cho việc chấm tự động.

# %%
plt.figure(figsize=(8, 6))

### BEGIN SOLUTION
sns.violinplot(x='target', y='petal length (cm)', data=df)
plt.title("Violin Plot of Petal Length")
plt.ylabel("Petal Length (cm)")
### END SOLUTION

plt.savefig('part_5.png')

# %% [markdown]
# # Bài Tập Để Kiểm Tra Hiểu Biết
# 
# Chúng ta đã hoàn thành các bước phân tích đơn biến trên bộ dữ liệu Iris. Hãy thử kiểm tra hiểu biết của bạn bằng cách trả lời câu hỏi sau:
# 
# 1. Phân tích đơn biến liên quan đến việc nghiên cứu:
# 
#    a) Một biến duy nhất
# 
#    b) Hai biến liên quan
# 
#    c) Nhiều biến cùng lúc
# 
# 
# 2. Biểu đồ nào thường được sử dụng để biểu diễn phân phối của một biến số?
# 
#    a) Biểu đồ hộp (box plot)
# 
#    b) Biểu đồ violin
# 
#    c) Biểu đồ histogram
# 
# 3. Biểu đồ nào thể hiện mật độ ước tính hạt nhân của một biến số?
# 
#    a) Biểu đồ hộp (box plot)
# 
#    b) Biểu đồ violin
# 
#    c) Biểu đồ KDE (kernel density estimate)
# 
# 4. Để biểu thị số lượng mẫu thuộc từng lớp trong bộ dữ liệu phân loại, ta sử dụng loại biểu đồ nào?
# 
#    a) Biểu đồ hộp (box plot)
# 
#    b) Biểu đồ count
# 
#    c) Biểu đồ violin
# 
# 5. Biểu đồ violin thường kết hợp giữa biểu đồ nào và biểu đồ nào?
# 
#    a) Histogram và bar plot
# 
#    b) Box plot và histogram
#    
#    c) Box plot và KDE plot
# 

# %%
# Trả lời theo định dạng mẫu sau, thay dấu chấm hỏi "?" bằng "a" hoặc "b" hoặc "c" theo lựa chọn của bạn
student_answers = ["a", "c", "c", "b", "b"]

# %% [markdown]
# # Phân tích đa biến
# Trong tuần này chúng ta sẽ tiến hành phân tích mối tương quan giữa các biến trong một bảng dữ liệu để thấy được mối quan hệ. Từ đó, cho phép chọn lựa các đặc trưng phù hợp để giải quyết bài toán.

# %% [markdown]
# Đầu tiên, tương tự như các bài tập trước, ta sẽ import những thư viện cần thiết

# %%
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# %% [markdown]
# Ở bài này, ta sẽ EDA những feature để phục vụ cho cho mô hình dự đoán (predict), trong đó:
# * Output (giá trị cần dự đoán) là cột Survived.
# * Các feature còn lại (ví dụ: Pclass, Name, Sex, Age) đóng vai trò là input của mô hình.
# 
# *Việc đầu tiên cần làm* là đọc file dữ liệu csv bằng thư viện pandas như những bài trước và lưu dataframe vào biến df

# %%
df = pd.read_csv("./data/train.csv")
df.head()

# %% [markdown]
# ## 1. Thống kê phân bố theo giá trị output
# 
# Tiếp theo, ta phân tích output feature có tên là Survived để quan sát tập giá trị đầu ra và phân bố của các giá trị này:

# %%
sns.countplot(x=df['Survived']);

# %% [markdown]
# Dựa vào biểu đồ ta có thể thấy, đây là bài toán phân loại nhị phân (binary classification) với hai nhãn đầu ra cần dự đoán là 0 và 1. Trong đó, số lượng nhãn 0 chiếm tỉ trọng cao hơn so với nhãn 1.
# 
# Tiếp đến, ta sẽ phân tích xem **những cột còn lại có liên quan gì đến cột Survived này hay không**.
# 
# Đầu tiên là thống kê phân bố của nhãn cần dự đoán theo cột Sex (giới tính):

# %%
sns.countplot(x=df['Sex'], hue=df['Survived']);

# %% [markdown]
# **Nhận xét**: Dựa vào biểu đồ trên, ta thấy tỉ lệ nữ còn sống so với nữ đã mất cao hơn tỉ lệ đó trên tập nam giới.
# 
# Ta thấy việc phân tích từng cột thế này **quá mất thời gian**, ta quyết định phân tích và hiển thị cùng lúc tất cả các cột có dạng dữ liệu Category hoặc những cột dạng Numerical mà ít labels như: [Pclass, Sex, SibSp, Parch, Embarked] bằng hàm countplot của thư viện seaborn và subplot của thư viện matplotlib.

# %% [markdown]
# Với từng subplot (của thư viện matplotlib), ta dùng hàm sns.countplot(x=..., hue=..., ax=...) để vẽ các biểu đồ phân tích. Trong đó cần phải truyền vào:
# * Tham số x là những cột theo thứ tự trong cols, và chạy từ trên xuống dưới, trái qua phải.
# * Tham số hue = cột 'Survived'
# 
# Các bạn cần hoàn thành bằng cách duyệt từ trên xuống dưới, từ trái qua phải các subplot để vẽ từng biểu đồ phân tích phân bố của giá trị output theo từng biến input đã nêu ở trên.
# 
# Vì số lượng ô của subplot là 2 dòng x 3 cột = 6 ô, trong khi số lượng đặc trưng cần hiển thị chỉ có 5, do đó ta cần phải xoá ô cuối cùng. Để xóa ô cuối cùng này ta sử dụng hàm set_visible. Đoạn code này đã được cung cấp ngay phía sau.

# %%
# Danh sách  5 cột cần hiển thị theo thứ tự
cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']

# Tạo subplot kích thước 2 x 3
fig, ax = plt.subplots(2, 3, figsize=(18, 18))
index = 0

### BEGIN SOLUTION
for i, col in enumerate(cols):
    # Tìm subplot hiện tại với vị trí hàng và cột
    row, col_index = divmod(i, 3)
    
    # Vẽ biểu đồ countplot cho cột hiện tại
    sns.countplot(x=col, hue='Survived', data=df, ax=ax[row, col_index])


### END SOLUTION

# Xóa ô cuối cùng vì chỉ có 5 cột cần hiển thị
ax.flat[-1].set_visible(False)
plt.tight_layout()

fig.savefig('part_1.png')

# %% [markdown]
# **Nhận xét**:
#   * Ở cột PClass:
#     - Class 3 có tỉ lệ chết (0) cao hơn đột biến so với tỉ lệ sống (1)
#     - Class 2 có tỉ lệ chết và sống gần như bằng nhau
#     - Class 1 có tỉ lệ sống cao hơn
#   * Ở cột SibSp:
#     - Đa số tỉ lệ chết (0) đều cao hơn tỉ lệ sống (1)
#     - Riêng ở cột có giá trị 1 thì tỉ lệ sống cao hơn đôi chút
#   * Ở cột Parch:
#     - Đa số tỉ lệ chết (0) đều cao hơn tỉ lệ sống (1)
#     - Riêng ở cột có giá trị 1 thì tỉ lệ sống cao hơn đôi chút
#   * Ở cột Embarked:
#     - Vẫn giống những cột ở trên, ở cột này tỉ lệ chết (0) vẫn trội hơn so với sống (1)
#   **Ta nhận thấy cột SibSp và Parch gần như giống nhau về sự ảnh hưởng đối với cột Survived*

# %% [markdown]
# Tiếp theo là những cột ['Age', 'Fare'], đây là dữ liệu dạng numerical (số), nếu ta dùng countplot như trên sẽ rất khó quan sát và phân tích:

# %%
sns.countplot(x=df['Age']);

# %% [markdown]
# Do đó, ở đây mình sẽ dùng hàm histplot của thư viện seaborn để phân tích. Trong đó, cần phải truyền vào:
# * Tham số x là cột Age
# * Tham số hue là cột Survived
# * Tham số kde=True: tham số để phân tích dạng đường phân bố liên tục.
# 
# Kết quả của việc phân tích này được lưu vào biến check1_sv = sns.histplot(...).
# 
# 
# 
# 
# 

# %%
plt.figure(figsize=(12, 12))
### BEGIN SOLUTION
check1_sv = sns.histplot(x='Age', hue='Survived', data=df, kde=True)
### END SOLUTION

plt.savefig('part_2.png')

# %% [markdown]
# **Nhận xét**: Ta có thể nhận xét, từ khoảng 0 đến 10 tuổi thì tỉ lệ sống sót cao hơn tỉ lệ mất.

# %% [markdown]
# Tương tự, ta sẽ phân tích cột Fare so với cột output Survived:

# %%
sns.histplot(x=df['Fare'], hue=df['Survived'], kde=True);

# %% [markdown]
# **Nhận xét**: Ta thấy nó khá khó nhìn nên ta sẽ dùng phương pháp binning Quantile.
# 
# Do đó, ta sẽ dùng hàm countplot của Seaborn để phân tích. Trong đó, cần phải truyền vào:
# * Tham số x=bins, với bins là kết quả của quá trình binning (đã được cung cấp)
# * Tham số hue= cột Survived
# 
# Kết quả của phân tích này ta lưu vào biến check2_sv.

# %%
labels = ['Cheap', 'Expensive', 'Luxury']
bins = pd.qcut(df['Fare'], 3, labels=labels)
sns.countplot(x=bins)

plt.figure(figsize=(12,12))

### BEGIN SOLUTION
check2_sv = sns.countplot(x=bins, hue='Survived', data=df)
### END SOLUTION

plt.savefig('part_3.png')

# %% [markdown]
# **Nhận xét:** Ta nhận thấy ở mức giá vé cao hơn thì tỉ lệ sống sót sẽ được tăng mạnh

# %% [markdown]
# ## 2. Tính toán Correlation

# %% [markdown]
# Ta xem lại dataframe và xem dataframe bị khuyết bao nhiêu dữ liệu

# %%
df

# %%
df.isna().sum()

# %% [markdown]
# * Ta sẽ phải xoá đi cột PassengerId và cột Name vì 2 cột này sẽ không thể ảnh hưởng đến việc sống hoặc chết
# * Ở khuôn khổ bài này, ta tạm thời không nói đến việc điền khuyết dữ liệu trống, nên những dòng bị khuyết dữ liệu, ta xoá đi
# * Ta dùng kĩ thuật Ordinal Encoding ở cột Sex và Embarked

# %%
### BEGIN SOLUTION
from sklearn.preprocessing import OrdinalEncoder

df = df.drop(columns=['PassengerId', 'Name'])
df = df.dropna()
encoder = OrdinalEncoder()
df[['Sex', 'Embarked']] = encoder.fit_transform(df[['Sex', 'Embarked']])
### END SOLUTION

# %% [markdown]
# Mục đích của việc làm trên là để biến dữ liệu dạng category thành numerical và bỏ hết giá trị NaN để tính toán correlation giữa các feature

# %%
heatmap_val = df.corr()
heatmap_val

# %% [markdown]
# Ta dùng hàm heatmap của thư viện seaborn để vẽ heatmap.

# %%
plt.figure(figsize=(12,12))
### BEGIN SOLUTION
new_order = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Survived']
df_reordered = df[new_order]

# Tính toán ma trận tương quan với thứ tự cột mới
corr_matrix = df_reordered.corr()
sns.heatmap(corr_matrix, annot=True)

### END SOLUTION

plt.savefig('part_4.png')

# %% [markdown]
# Không ngoài dự đoán, 2 feature SibSp và Parch có độ tương đồng cao nhất (0.38) trong các feature






